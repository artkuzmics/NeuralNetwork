{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from models.NeuralNetwork import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"adult.csv\"\n",
    "data = read_csv(filename,delimiter=\",\", na_values=\"?\")\n",
    "\n",
    "#class_counts = data.groupby('income').size()\n",
    "#print(class_counts)\n",
    "\n",
    "\"\"\"\n",
    "Imputation\n",
    "\"\"\"\n",
    "null_columns =['workclass','occupation','native.country']\n",
    "for i in null_columns:\n",
    "    data.fillna(data[i].mode()[0], inplace=True)\n",
    "    \n",
    "\"\"\"\n",
    "Factorize\n",
    "\"\"\"  \n",
    "data['income'] = data['income'].replace({'<=50K':0, '>50K':1})\n",
    "data['sex'] = data['sex'].replace({'Female':0, 'Male':1})\n",
    "data['race'] = data['race'].replace({'White':0, 'Black':1, 'Asian-Pac-Islander':2, 'Other':3,'Amer-Indian-Eskimo':4})\n",
    "data['workclass'] = data['workclass'].replace({'Private':0, 'State-gov':1, 'Federal-gov':2, 'Self-emp-not-inc':3,\n",
    "       'Self-emp-inc':4, 'Local-gov':5, 'Without-pay':6, 'Never-worked':7})\n",
    "data['native.country'] = data['native.country'].replace({'United-States':0, 'Private':1, 'Mexico':2, 'Greece':3, 'Vietnam':4, 'China':5,\n",
    "       'Taiwan':6, 'India':7, 'Philippines':8, 'Trinadad&Tobago':9, 'Canada':10,\n",
    "       'South':11, 'Holand-Netherlands':12, 'Puerto-Rico':13, 'Poland':14, 'Iran':15,\n",
    "       'England':16, 'Germany':17, 'Italy':18, 'Japan':19, 'Hong':20, 'Honduras':21, 'Cuba':22,\n",
    "       'Ireland':23, 'Cambodia':24, 'Peru':25, 'Nicaragua':26, 'Dominican-Republic':27,\n",
    "       'Haiti':28, 'El-Salvador':29, 'Hungary':30, 'Columbia':31, 'Guatemala':32,\n",
    "       'Jamaica':33, 'Ecuador':34, 'France':35, 'Yugoslavia':36, 'Scotland':37,\n",
    "       'Portugal':38, 'Laos':39, 'Thailand':40, 'Outlying-US(Guam-USVI-etc)':41})\n",
    "data['occupation'] = data['occupation'].replace({'Private':0, 'Exec-managerial':1, 'Machine-op-inspct':2,\n",
    "       'Prof-specialty':3, 'Other-service':4, 'Adm-clerical':5, 'Craft-repair':6,\n",
    "       'Transport-moving':7, 'Handlers-cleaners':8, 'Sales':9,\n",
    "       'Farming-fishing':10, 'Tech-support':11, 'Protective-serv':12,\n",
    "       'Armed-Forces':13, 'Priv-house-serv':14})\n",
    "data['relationship'] = data['relationship'].replace({'Not-in-family':0, 'Unmarried':1, 'Own-child':2, 'Other-relative':3,\n",
    "       'Husband':4, 'Wife':5})\n",
    "data['education'] = data['education'].replace({'HS-grad':0, 'Some-college':1, '7th-8th':2, '10th':3, 'Doctorate':4,\n",
    "       'Prof-school':5, 'Bachelors':6, 'Masters':7, '11th':8, 'Assoc-acdm':9,\n",
    "       'Assoc-voc':10, '1st-4th':11, '5th-6th':12, '12th':13, '9th':14, 'Preschool':15})\n",
    "data['marital.status'] = data['marital.status'].replace(['Never-married', 'Divorced', 'Separated', 'Widowed'], 'Single')\n",
    "data['marital.status'] = data['marital.status'].replace(['Married-civ-spouse', 'Married-spouse-absent', 'Married-AF-spouse'], 'Married')\n",
    "data['marital.status'] = data['marital.status'].map({'Married':1, 'Single':0})\n",
    "\n",
    "\n",
    "X = data.iloc[:,:14]\n",
    "y = data.iloc[:,14]\n",
    "\n",
    "\"\"\"\n",
    "Undersample\n",
    "\"\"\"  \n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X, y = undersample.fit_resample(X,y)\n",
    "\n",
    "\"\"\"\n",
    "Scaling\n",
    "\"\"\" \n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "\"\"\"\n",
    "Splitting\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(rescaledX,y,test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(data,batch_size):\n",
    "    st_batch = data.shape[0] // batch_size\n",
    "    idx_end = st_batch * batch_size\n",
    "    batch = np.split(data[:idx_end], st_batch)\n",
    "    \n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        batch += [data[idx_end:]]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - val_acc: 50.27096, val_loss: nan\n",
      "epoch: 100 - val_acc: 50.27096, val_loss: nan\n",
      "epoch: 200 - val_acc: 50.27096, val_loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-53e03b6f6446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_cross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/Coursework/NeuralNetwork/models/NeuralNetwork.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, Y, f)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDelta3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdCost\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDelta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDelta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDelta3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(x_no = X.shape[1], y_no=1, h_no=3)\n",
    "\n",
    "cost_log = []\n",
    "epochs = 500\n",
    "batch_size = 100\n",
    "learning_rate = 1\n",
    "\n",
    "X_batch = sample_batch(X_train, batch_size)\n",
    "Y_batch = sample_batch(y_train.to_numpy(),batch_size)\n",
    "\n",
    "for t in range(epochs):\n",
    "    for xs,ys in zip(X_batch,Y_batch):\n",
    "        \n",
    "        nn.forward(xs)\n",
    "        nn.backward(ys, nn.db_cross_entropy)\n",
    "        \n",
    "        nn.gd(lr=1)\n",
    "        \n",
    "    _, prediction, accuracy = nn.predict(X_test,y_test.to_numpy())\n",
    "    val_loss = np.mean(nn.b_cross_entropy(y_test.to_numpy(),_))\n",
    "    #cost = np.mean(nn.b_cross_entropy(ys, nn.A3))\n",
    "    cost_log.append([t,val_loss])\n",
    "    if t % 100 == 0:\n",
    "        #print(f\"epoch: {t} - loss: {round(cost,5)}\", end=\" \")\n",
    "        print(f\"epoch: {t} - val_acc: {round(accuracy,5)}, val_loss: {round(val_loss,5)}\")\n",
    "        \n",
    "cost_log = pd.DataFrame(cost_log, columns=[\"epochs\",\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.plot(cost_log[\"epochs\"],cost_log[\"loss\"], c=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. ... 0. 1. 1.]\n",
      "80.29964934650941\n"
     ]
    }
   ],
   "source": [
    "_, prediction, accuracy = nn.predict(X_test,y_test.to_numpy())\n",
    "print(prediction)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
